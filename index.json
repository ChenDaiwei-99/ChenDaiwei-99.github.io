[{"content":" Unraveling The Impact of Training Samples\nDaiwei Chen, Jane Zhang, and Ramya Korlakai Vinayak. The Third Blogpost Track at ICLR 2024. ICLR Blogpost Learning Capacity: A Measure of the Effective Dimensionality of a Model\nDaiwei Chen, Weikai Chang, Pratik Chaudhari. Arxiv, Preprint, May 2023 Arxiv ","date":"1 February 2024","permalink":"/publications/","section":"Welcome to Daiwei's personal website","summary":"Unraveling The Impact of Training Samples\nDaiwei Chen, Jane Zhang, and Ramya Korlakai Vinayak. The Third Blogpost Track at ICLR 2024. ICLR Blogpost Learning Capacity: A Measure of the Effective Dimensionality of a Model","title":"Publications"},{"content":"","date":"31 January 2024","permalink":"/tags/linear-algebra/","section":"Tags","summary":"","title":"Linear Algebra"},{"content":" Click here to see my notes ","date":"31 January 2024","permalink":"/blogs/linear-algebra-refresh/","section":"Research Blogs","summary":" Click here to see my notes ","title":"Linear Algebra Refresh"},{"content":" My daily study / research / coding notes ","date":"31 January 2024","permalink":"/blogs/","section":"Research Blogs","summary":" My daily study / research / coding notes ","title":"Research Blogs"},{"content":"","date":"31 January 2024","permalink":"/tags/singular-value-decomposition-svd/","section":"Tags","summary":"","title":"Singular Value Decomposition (SVD)"},{"content":" ","date":"31 January 2024","permalink":"/tags/","section":"Tags","summary":" ","title":"Tags"},{"content":"Think deeper, Fail faster!\nAbout Me # I'm a first year Electrical and Computer Engineering (ECE) PhD in Machine Learning and Optimization Theory (MLOPT) Research Group at the University of Wisconsin-Madison. Under the guidance of my advisor, Prof. Ramya Korlakai Vinayak, my research focuses on AI alignment, Foundation Models, Representation Learning, and Machine Learning Theory. Before starting my PhD journey, I obtained my Master\u0026rsquo;s degree in Systems Engineering from the University of Pennsylvania under the guidance of Prof. Pratik Chaudhari, and received my Bachelor of Science in Psychology from Zhejiang University.\nNews # [2024/02/16] Our blogpost Unraveling The Impact of Training Samples has been accepted by ICLR! [2023/08/14] Arrive at Madison and Start my PhD research journey! ","date":"31 January 2024","permalink":"/","section":"Welcome to Daiwei's personal website","summary":"Think deeper, Fail faster!\nAbout Me # I'm a first year Electrical and Computer Engineering (ECE) PhD in Machine Learning and Optimization Theory (MLOPT) Research Group at the University of Wisconsin-Madison.","title":"Welcome to Daiwei's personal website"},{"content":"","date":"8 January 2024","permalink":"/tags/coding/","section":"Tags","summary":"","title":"Coding"},{"content":"","date":"8 January 2024","permalink":"/tags/machine-learning-ml/","section":"Tags","summary":"","title":"Machine Learning (ML)"},{"content":"","date":"8 January 2024","permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"PyTorch"},{"content":"1. Categorical Reparameterization with Gumbel-Softmax (create categorical variables in neural networks) # Background: Discrete variables are important in neural networks. E.g. discrete variables have been used to learn probabilistic latent representations that correspond to distinct semantic classes. Contributions Gumbel-Softmax, a continuous distribution on the simplex that can approximate categorical samples. this paper provides a simple, differentiable approximate sampling mechanism for categorical variables that can be integrated into neural networks and trained using standard back-propagation. The Gumbel-Softmax Distribution # Gumble Softmax Estimator # For learning ,there is a tradeoff between small temperatures, where samples are close to one-hot but the variance of the gradients is large, and large temperatures, where samples are smooth but the variance of the gradient is small.\nIn practice, we start at a high temperature and anneal to a small but non-zero temperature.\nIncorporates Noise: Gumbel-Softmax incorporates Gumbel noise into the input, which allows the model to explore a variety of outputs, making it stochastic as opposed to the deterministic nature of Softmax. ","date":"8 January 2024","permalink":"/blogs/pytorch-tricks/","section":"Research Blogs","summary":"1. Categorical Reparameterization with Gumbel-Softmax (create categorical variables in neural networks) # Background: Discrete variables are important in neural networks. E.g. discrete variables have been used to learn probabilistic latent representations that correspond to distinct semantic classes.","title":"PyTorch tricks"},{"content":"","date":"6 January 2024","permalink":"/tags/direct-preference-optimization-dpo/","section":"Tags","summary":"","title":"Direct Preference Optimization (DPO)"},{"content":"","date":"6 January 2024","permalink":"/tags/large-language-models-llms/","section":"Tags","summary":"","title":"Large Language Models (LLMs)"},{"content":" Deep Reinforcement Learning from Human Preferences # Summary: Compared with the typical method of complex RL systems (which manually set complex goals), this paper explore another approach which define the goals just as human preferences between pairs of trajectory segments (learn a reward function from human feedback). This approach can effectively solve complex RL tasks without hand-engineer reward function. Also, this approach can solve the big issue about the misalignment between our values and the objectives of our RL systems. Methods fit the reward function with human preference data (only query the trajectories that exhibit high variance across the ensembles of the reward functions) based on the Bradley-Terry model, assume human’s probability of preferring a segment depends exponentially on the value of the latent reward. DPO Direct Preference Optimization: Your Language Model is Secretly a Reward Model # Summary: This paper proposes an reparameterization idea to directly optimize a LM to align with human preferences, without fitting a reward function or using reinforcement learning. This paper also provides theoretical analysis to show that the reparameterization won’t constrain the class of learned reward models.\nDefinitions\nRLHF pipeline: 1. SFT Phase; 2. Reward Modelling Phase; 3. RL Fine-Tuning Phase DPO: Direct Preference Optimization - by deducing the optimal policy of the RLHF and using the property of the Bradley-Terry model (i.e. we only need the reward difference to calculate the preference probability), we can eliminate the need of fitting a reward function and change the loss function from the original expression of reward function to the expression of optimal policies by designing the reparameterization of the reward function. Rewarded soups: Towards Pareto-optimal Alignment by Interpolating Weights Fine-tuned on Diverse Rewards # Summary: 1) Optimize a set of N weights (from the same pre-trained model) which corresponds to N different proxy rewards. 2) Linearly interpolate the weights to get a continuous set of reward soups. The paper constructs sufficient experiments over foundation models, and demonstrates that rewarded soup can mitigate reward misspecification.\nDefinitions:\nLinear Mode Connectivity (LMC) \u0026amp; Weight Interpolations (WI): weights fine-tuned from a shared pre-trained initialization remain linearly connected and thus can be interpolated. (In other words, when the LMC holds, combining networks in weights combines their abilities.) Multiple-Objective Reinforcement Learning (MORL): We have N different proxy rewards. By interpolating them using M different weightings, we could train M different models. Then, based on the user’s specific reward, we choose one of the M models that maximize the user’s reward. Reward Misspecification: during the reward optimization process of LLMs, we can only optimize a proxy reward (not true reward during test time) during training. However, we don’t actually know the true reward when we apply the models in real world. Such model misspecification may hinder the alignment of the network. Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization # Summary: Proposes Multi-Objective DPO (MODPO), an RL-free algorithm that extends DPO for multiple alignment objectives with minimal overheads. Key idea: by doing DPO math deduction on the multiple objective formula, they find out that compared with the typical DPO, we only need to train one margin reward model to optimize the loss of MODPO. The results show that this method can achieve the comparable performance to MORLHF and much better than reward soups or loss weighting.\nTypical approach for multiple LMs\nDivide human feedback into multiple fine-grained dimensions Create distinct reward models Optimize different LMs with different reward weightings during fine-tuning phase Discovering Language Model Behaviors with Model-Written Evaluations # Summary:\nThis paper provides a method to automatically generate evaluation datasets by leveraging LMs (Language Models) and PMs (Preference Models). By using this proposed method, we can evaluate the behaviors of LMs without the need of human-written evaluation datasets (which usually cost large amounts of human efforts). By testing the generated 154 evaluation datasets, this paper finds new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialogue user’s preferred answer (“sycophancy”) and express greater desire to pursue concerning goals like resource acquisition and goal preservation. More RLHF makes LMs worse, e.g. RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Definitions:\nEvaluation datasets: the datasets used to evaluate the behaviors of LMs, such as Gender bias test, Stated desire for power test and Ends-Justify-Means reasoning test. Preference models: the model used for RLHF training. This model is trained to mimic the human preference between two replies from LMs. Inverse scaling: the behaviors of LMs become worse, as the size of LMs increases. Methods: ","date":"6 January 2024","permalink":"/blogs/llm-rlhf-paper-reading-notes/","section":"Research Blogs","summary":"Deep Reinforcement Learning from Human Preferences # Summary: Compared with the typical method of complex RL systems (which manually set complex goals), this paper explore another approach which define the goals just as human preferences between pairs of trajectory segments (learn a reward function from human feedback).","title":"LLM \u0026 RLHF - Paper Reading Notes"},{"content":"","date":"6 January 2024","permalink":"/tags/preference-learning/","section":"Tags","summary":"","title":"Preference Learning"},{"content":"Some coding practice about multiple users metric learning and preference learning.\nChenDaiwei-99/multiuser-metric-preference Paper code for \u0026ldquo;One for All: Simultaneous Metric and Preference Learning over Multiple Users\u0026rdquo; Jupyter Notebook 0 0 ","date":"5 January 2024","permalink":"/blogs/code-practice-multiple-metric-preference-learning/","section":"Research Blogs","summary":"Some coding practice about multiple users metric learning and preference learning.\nChenDaiwei-99/multiuser-metric-preference Paper code for \u0026ldquo;One for All: Simultaneous Metric and Preference Learning over Multiple Users\u0026rdquo; Jupyter Notebook 0 0 ","title":"Coding Practice"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":" Badminton / Running / Guitar / Video games! ","date":"1 January 0001","permalink":"/life/","section":"Personal Life","summary":" Badminton / Running / Guitar / Video games! ","title":"Personal Life"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"}]