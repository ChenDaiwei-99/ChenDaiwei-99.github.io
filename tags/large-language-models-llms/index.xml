<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Large Language Models (LLMs) on Daiwei Chen</title>
    <link>http://ChenDaiwei-99.github.io/tags/large-language-models-llms/</link>
    <description>Recent content in Large Language Models (LLMs) on Daiwei Chen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Â© 2024 Daiwei Chen :thinking_face:</copyright>
    <lastBuildDate>Sat, 06 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://ChenDaiwei-99.github.io/tags/large-language-models-llms/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM &amp; RLHF - Paper Reading Notes</title>
      <link>http://ChenDaiwei-99.github.io/blogs/llm-rlhf-paper-reading-notes/</link>
      <pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>http://ChenDaiwei-99.github.io/blogs/llm-rlhf-paper-reading-notes/</guid>
      <description>Deep Reinforcement Learning from Human Preferences # Summary: Compared with the typical method of complex RL systems (which manually set complex goals), this paper explore another approach which define the goals just as human preferences between pairs of trajectory segments (learn a reward function from human feedback).</description>
      
    </item>
    
  </channel>
</rss>
